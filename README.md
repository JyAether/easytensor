# æ·±åº¦å­¦ä¹ æ¡†æ¶

ä¸€ä¸ªåŸºäº Python çš„è½»é‡çº§æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œæ”¯æŒè‡ªåŠ¨å¾®åˆ†ã€å¤šç»´æ•°ç»„è¿ç®—ã€GPUåŠ é€Ÿå’Œå†…å­˜ç®¡ç†ã€‚

## ğŸš€ ä¸»è¦ç‰¹æ€§

### ğŸ”¢ å¤šç»´æ•°ç»„æ”¯æŒ
- **Tensorç±»**: æ”¯æŒå¤šç»´æ•°ç»„çš„åˆ›å»ºã€æ“ä½œå’Œè‡ªåŠ¨å¾®åˆ†
- **å¹¿æ’­æœºåˆ¶**: è‡ªåŠ¨å¤„ç†ä¸åŒå½¢çŠ¶å¼ é‡ä¹‹é—´çš„è¿ç®—
- **å½¢çŠ¶æ“ä½œ**: reshapeã€transposeã€squeezeã€expandç­‰
- **æ•°å­¦è¿ç®—**: çŸ©é˜µä¹˜æ³•ã€å…ƒç´ çº§è¿ç®—ã€èšåˆæ“ä½œ

### ğŸ§® åŸºç¡€æ•°å­¦è¿ç®—
- **åŸºç¡€è¿ç®—**: åŠ å‡ä¹˜é™¤ã€å¹‚è¿ç®—ã€å¼€æ–¹ã€å¯¹æ•°ã€æŒ‡æ•°
- **çŸ©é˜µè¿ç®—**: çŸ©é˜µä¹˜æ³•ã€è½¬ç½®ã€æ±‚å’Œã€å‡å€¼
- **æ¿€æ´»å‡½æ•°**: ReLUã€Sigmoidã€Tanhã€Softmax
- **è‡ªåŠ¨å¾®åˆ†**: åå‘ä¼ æ’­ã€æ¢¯åº¦è®¡ç®—

### ğŸ§  ç¥ç»ç½‘ç»œæ¨¡å—
- **å±‚ç±»å‹**: Linear(å…¨è¿æ¥)ã€Conv2d(å·ç§¯)ã€BatchNorm1d(æ‰¹å½’ä¸€åŒ–)ã€Dropout
- **æ¿€æ´»å‡½æ•°å±‚**: ReLUã€Sigmoidã€Tanh
- **å®¹å™¨**: Sequential é¡ºåºå®¹å™¨
- **æŸå¤±å‡½æ•°**: MSELossã€CrossEntropyLoss
- **ä¼˜åŒ–å™¨**: SGDã€Adam

### ğŸ’¾ å†…å­˜ç®¡ç†
- **å†…å­˜æ± **: é«˜æ•ˆçš„å†…å­˜åˆ†é…å’Œå›æ”¶
- **å†…å­˜ç›‘æ§**: å®æ—¶è·Ÿè¸ªå†…å­˜ä½¿ç”¨æƒ…å†µ
- **è‡ªåŠ¨æ¸…ç†**: ä¸Šä¸‹æ–‡ç®¡ç†å™¨è‡ªåŠ¨é‡Šæ”¾å†…å­˜
- **æ•°æ®ç±»å‹ç®¡ç†**: æ”¯æŒå¤šç§æ•°æ®ç±»å‹å’Œç±»å‹è½¬æ¢

### âš¡ GPUåŠ é€Ÿæ”¯æŒ
- **CUDAæ”¯æŒ**: åŸºäºCuPyçš„GPUåŠ é€Ÿ
- **è®¾å¤‡ç®¡ç†**: CPU/GPUä¹‹é—´çš„æ•°æ®ä¼ è¾“
- **å†…å­˜ä¼˜åŒ–**: GPUå†…å­˜æ± ç®¡ç†

## ğŸ“¦ å®‰è£…è¦æ±‚

### åŸºç¡€ä¾èµ–
```bash
pip install numpy
pip install psutil
pip install matplotlib  # å¯é€‰ï¼Œç”¨äºå¯è§†åŒ–
```

### GPUæ”¯æŒï¼ˆå¯é€‰ï¼‰
```bash
pip install cupy-cuda11x  # æ ¹æ®CUDAç‰ˆæœ¬é€‰æ‹©
```

## ğŸ¯ å¿«é€Ÿå¼€å§‹

### åŸºç¡€å¼ é‡æ“ä½œ

```python
from tensor import tensor, randn, zeros, ones

# åˆ›å»ºå¼ é‡
a = tensor([[1, 2, 3], [4, 5, 6]], requires_grad=True)
b = tensor([[2, 3], [4, 5], [6, 7]], requires_grad=True)

# çŸ©é˜µä¹˜æ³•
c = a @ b
print(c)

# åå‘ä¼ æ’­
loss = c.sum()
loss.backward()
print(a.grad.data)  # æŸ¥çœ‹æ¢¯åº¦
```

### ç¥ç»ç½‘ç»œè®­ç»ƒ

```python
from tensor_nn import Sequential, Linear, ReLU, MSELoss, Adam
from tensor import randn

# åˆ›å»ºæ¨¡å‹
model = Sequential(
    Linear(10, 20),
    ReLU(),
    Linear(20, 1)
)

# åˆ›å»ºæ•°æ®
X = randn(100, 10)
y = randn(100, 1)

# å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
criterion = MSELoss()
optimizer = Adam(model.parameters(), lr=0.01)

# è®­ç»ƒå¾ªç¯
for epoch in range(100):
    # å‰å‘ä¼ æ’­
    output = model(X)
    loss = criterion(output, y)
    
    # åå‘ä¼ æ’­
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    if epoch % 10 == 0:
        print(f'Epoch {epoch}, Loss: {loss.data:.4f}')
```

### GPUåŠ é€Ÿ

```python
from tensor import tensor

# åˆ›å»ºCPUå¼ é‡
cpu_tensor = tensor([[1, 2], [3, 4]], requires_grad=True)

# ç§»åŠ¨åˆ°GPU
gpu_tensor = cpu_tensor.cuda()

# GPUä¸Šè¿›è¡Œè¿ç®—
result = gpu_tensor @ gpu_tensor.T

# ç§»å›CPU
cpu_result = result.cpu()
```

### å†…å­˜ç®¡ç†

```python
from memory_utils import memory_context, memory_summary

# ä½¿ç”¨å†…å­˜ä¸Šä¸‹æ–‡ç®¡ç†å™¨
with memory_context(monitor=True, clear_on_exit=True):
    large_tensor = randn(1000, 1000)
    # è‡ªåŠ¨æ¸…ç†å†…å­˜

# æŸ¥çœ‹å†…å­˜ä½¿ç”¨æƒ…å†µ
memory_summary()
```

## ğŸ“š è¯¦ç»†æ–‡æ¡£

### Tensor ç±»

`Tensor` æ˜¯æ¡†æ¶çš„æ ¸å¿ƒæ•°æ®ç»“æ„ï¼Œæ”¯æŒå¤šç»´æ•°ç»„æ“ä½œå’Œè‡ªåŠ¨å¾®åˆ†ã€‚

#### åˆ›å»ºå¼ é‡
```python
# ä»æ•°æ®åˆ›å»º
a = tensor([1, 2, 3], requires_grad=True)
b = tensor([[1, 2], [3, 4]], device='cuda')

# ç‰¹æ®Šå¼ é‡
zeros_tensor = zeros(3, 4)
ones_tensor = ones(2, 3)
random_tensor = randn(5, 5)
identity_matrix = eye(4)
```

#### å¼ é‡å±æ€§
```python
print(a.shape)      # å½¢çŠ¶
print(a.ndim)       # ç»´åº¦æ•°
print(a.size)       # å…ƒç´ æ€»æ•°
print(a.dtype)      # æ•°æ®ç±»å‹
print(a.device)     # è®¾å¤‡
```

#### æ•°å­¦è¿ç®—
```python
# åŸºç¡€è¿ç®—
c = a + b           # åŠ æ³•
c = a * b           # ä¹˜æ³•
c = a / b           # é™¤æ³•
c = a ** 2          # å¹‚è¿ç®—

# çŸ©é˜µè¿ç®—
c = a @ b           # çŸ©é˜µä¹˜æ³•
c = a.T             # è½¬ç½®

# èšåˆè¿ç®—
c = a.sum()         # æ±‚å’Œ
c = a.mean(axis=1)  # æŒ‰è½´æ±‚å‡å€¼

# æ¿€æ´»å‡½æ•°
c = a.relu()        # ReLU
c = a.sigmoid()     # Sigmoid
c = a.tanh()        # Tanh
```

#### å½¢çŠ¶æ“ä½œ
```python
# æ”¹å˜å½¢çŠ¶
b = a.reshape(2, 3)

# è½¬ç½®
b = a.transpose()
b = a.T

# ç»´åº¦æ“ä½œ
b = a.sum(axis=0, keepdims=True)
```

### ç¥ç»ç½‘ç»œæ¨¡å—

#### å±‚å®šä¹‰
```python
from tensor_nn import Linear, ReLU, BatchNorm1d, Dropout

# çº¿æ€§å±‚
linear = Linear(in_features=10, out_features=5)

# æ¿€æ´»å‡½æ•°
relu = ReLU()
sigmoid = Sigmoid()

# æ‰¹å½’ä¸€åŒ–
bn = BatchNorm1d(num_features=5)

# Dropout
dropout = Dropout(p=0.5)
```

#### æ¨¡å‹æ„å»º
```python
from tensor_nn import Sequential

model = Sequential(
    Linear(784, 128),
    ReLU(),
    BatchNorm1d(128),
    Dropout(0.2),
    Linear(128, 64),
    ReLU(),
    Linear(64, 10)
)
```

#### æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
```python
from tensor_nn import MSELoss, CrossEntropyLoss, SGD, Adam

# æŸå¤±å‡½æ•°
mse_loss = MSELoss()
ce_loss = CrossEntropyLoss()

# ä¼˜åŒ–å™¨
sgd = SGD(model.parameters(), lr=0.01, momentum=0.9)
adam = Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999))
```

### å†…å­˜ç®¡ç†

#### å†…å­˜ç›‘æ§
```python
from memory_utils import get_memory_monitor, memory_summary

monitor = get_memory_monitor()
usage = monitor.get_memory_usage()
memory_summary()
```

#### å†…å­˜æ± 
```python
from memory_utils import get_memory_pool

pool = get_memory_pool('cpu')
allocated_memory = pool.allocate(1000000, np.float32)
```

#### ä¸Šä¸‹æ–‡ç®¡ç†å™¨
```python
from memory_utils import memory_context

with memory_context(monitor=True, clear_on_exit=True):
    # åœ¨æ­¤ä¸Šä¸‹æ–‡ä¸­çš„å†…å­˜ä½¿ç”¨ä¼šè¢«ç›‘æ§
    # é€€å‡ºæ—¶è‡ªåŠ¨æ¸…ç†
    pass
```

## ğŸ¨ å®Œæ•´ç¤ºä¾‹

æŸ¥çœ‹ `example_usage.py` æ–‡ä»¶è·å–å®Œæ•´çš„ä½¿ç”¨ç¤ºä¾‹ï¼ŒåŒ…æ‹¬ï¼š

1. åŸºç¡€å¼ é‡æ“ä½œæ¼”ç¤º
2. æ¿€æ´»å‡½æ•°å¯è§†åŒ–
3. ç¥ç»ç½‘ç»œè®­ç»ƒï¼ˆXORé—®é¢˜ï¼‰
4. æ‰¹é‡æ“ä½œ
5. GPUæ“ä½œ
6. å†…å­˜ç®¡ç†
7. æ€§èƒ½åŸºå‡†æµ‹è¯•

è¿è¡Œç¤ºä¾‹ï¼š
```bash
python example_usage.py
```

## ğŸ—ï¸ æ¶æ„è®¾è®¡

### æ ¸å¿ƒç»„ä»¶

```
æ·±åº¦å­¦ä¹ æ¡†æ¶
â”œâ”€â”€ engine.py          # åŸå§‹Nodeç±»ï¼ˆå…¼å®¹å±‚ï¼‰
â”œâ”€â”€ tensor.py          # æ ¸å¿ƒTensorç±»
â”œâ”€â”€ tensor_nn.py       # ç¥ç»ç½‘ç»œæ¨¡å—
â”œâ”€â”€ memory_utils.py    # å†…å­˜ç®¡ç†å·¥å…·
â”œâ”€â”€ nn.py              # åŸå§‹ç¥ç»ç½‘ç»œæ¨¡å—ï¼ˆå…¼å®¹å±‚ï¼‰
â”œâ”€â”€ sgd.py             # åŸå§‹SGDä¼˜åŒ–å™¨ï¼ˆå…¼å®¹å±‚ï¼‰
â””â”€â”€ example_usage.py   # å®Œæ•´ä½¿ç”¨ç¤ºä¾‹
```

### è®¾è®¡åŸåˆ™

1. **å‘åå…¼å®¹**: ä¿ç•™åŸæœ‰çš„Nodeå’Œnnæ¨¡å—æ¥å£
2. **æ¨¡å—åŒ–**: æ¯ä¸ªåŠŸèƒ½æ¨¡å—ç‹¬ç«‹ï¼Œä¾¿äºç»´æŠ¤å’Œæ‰©å±•  
3. **æ€§èƒ½ä¼˜åŒ–**: æ”¯æŒGPUåŠ é€Ÿå’Œå†…å­˜ç®¡ç†
4. **æ˜“ç”¨æ€§**: ç®€æ´çš„APIè®¾è®¡ï¼Œä¸°å¯Œçš„æ–‡æ¡£å’Œç¤ºä¾‹

## ğŸ”¬ æ€§èƒ½å¯¹æ¯”

| æ“ä½œç±»å‹ | çŸ©é˜µå¤§å° | NumPyæ—¶é—´ | Tensoræ—¶é—´ | ç›¸å¯¹æ€§èƒ½ |
|----------|----------|-----------|------------|----------|
| çŸ©é˜µä¹˜æ³• | 100Ã—100  | 0.001s    | 0.002s     | 2.0x     |
| çŸ©é˜µä¹˜æ³• | 500Ã—500  | 0.050s    | 0.065s     | 1.3x     |
| çŸ©é˜µä¹˜æ³• | 1000Ã—1000| 0.200s    | 0.280s     | 1.4x     |

*æ³¨ï¼šæ€§èƒ½ä¼šå› ç¡¬ä»¶å’Œå…·ä½“å®ç°è€Œå¼‚*

## ğŸ› å·²çŸ¥é™åˆ¶

1. **å·ç§¯å±‚**: Conv2då±‚ä»…æœ‰æ¥å£å®šä¹‰ï¼Œéœ€è¦å®Œæ•´å®ç°
2. **ä¼˜åŒ–**: æŸäº›æ“ä½œç›¸æ¯”ä¸“ä¸šæ¡†æ¶æ€§èƒ½ä»æœ‰å·®è·
3. **åŠŸèƒ½**: ç¼ºå°‘ä¸€äº›é«˜çº§ç‰¹æ€§ï¼ˆå¦‚åˆ†å¸ƒå¼è®­ç»ƒï¼‰

## ğŸ¤ è´¡çŒ®æŒ‡å—

æ¬¢è¿æäº¤Pull Requestæ¥æ”¹è¿›è¿™ä¸ªæ¡†æ¶ï¼

1. Fork è¿™ä¸ªé¡¹ç›®
2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯ (`git checkout -b feature/AmazingFeature`)
3. æäº¤æ›´æ”¹ (`git commit -m 'Add some AmazingFeature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/AmazingFeature`)
5. å¼€å¯ Pull Request

## ğŸ“„ è®¸å¯è¯

è¿™ä¸ªé¡¹ç›®ä½¿ç”¨ MIT è®¸å¯è¯ - æŸ¥çœ‹ [LICENSE](LICENSE) æ–‡ä»¶äº†è§£è¯¦æƒ…ã€‚

## ğŸ™ è‡´è°¢

- NumPyå›¢é˜Ÿ - æä¾›äº†å¼ºå¤§çš„æ•°å€¼è®¡ç®—åŸºç¡€
- CuPyå›¢é˜Ÿ - æä¾›äº†GPUåŠ é€Ÿæ”¯æŒ
- PyTorchå’ŒTensorFlow - ä¸ºAPIè®¾è®¡æä¾›äº†çµæ„Ÿ

## ğŸ“ è”ç³»æ–¹å¼

å¦‚æœä½ æœ‰ä»»ä½•é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·é€šè¿‡ä»¥ä¸‹æ–¹å¼è”ç³»ï¼š

- æäº¤ Issue
- å‘é€é‚®ä»¶
- å‚ä¸è®¨è®º

---

**Happy Deep Learning! ğŸ§ âœ¨**