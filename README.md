# EasyTensor

ä¸€ä¸ªåŸºäº Python çš„è½»é‡çº§æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œæ”¯æŒè‡ªåŠ¨å¾®åˆ†ã€å¤šç»´æ•°ç»„è¿ç®—ã€GPUåŠ é€Ÿå’Œå†…å­˜ç®¡ç†ã€‚EasyTensor æä¾›äº†ç±»ä¼¼ PyTorch çš„ API è®¾è®¡ï¼Œè®©æ·±åº¦å­¦ä¹ å˜å¾—æ›´åŠ ç®€å•æ˜“ç”¨ã€‚

## ğŸš€ ä¸»è¦ç‰¹æ€§

### ğŸ”¢ å¤šç»´æ•°ç»„æ”¯æŒ
- **Tensorç±»**: æ”¯æŒå¤šç»´æ•°ç»„çš„åˆ›å»ºã€æ“ä½œå’Œè‡ªåŠ¨å¾®åˆ†
- **å¹¿æ’­æœºåˆ¶**: è‡ªåŠ¨å¤„ç†ä¸åŒå½¢çŠ¶å¼ é‡ä¹‹é—´çš„è¿ç®—
- **å½¢çŠ¶æ“ä½œ**: reshapeã€transposeã€squeezeã€expandç­‰
- **æ•°å­¦è¿ç®—**: çŸ©é˜µä¹˜æ³•ã€å…ƒç´ çº§è¿ç®—ã€èšåˆæ“ä½œ

### ğŸ§® åŸºç¡€æ•°å­¦è¿ç®—
- **åŸºç¡€è¿ç®—**: åŠ å‡ä¹˜é™¤ã€å¹‚è¿ç®—ã€å¼€æ–¹ã€å¯¹æ•°ã€æŒ‡æ•°
- **çŸ©é˜µè¿ç®—**: çŸ©é˜µä¹˜æ³•ã€è½¬ç½®ã€æ±‚å’Œã€å‡å€¼
- **æ¿€æ´»å‡½æ•°**: ReLUã€Sigmoidã€Tanhã€Softmax
- **è‡ªåŠ¨å¾®åˆ†**: åå‘ä¼ æ’­ã€æ¢¯åº¦è®¡ç®—

### ğŸ§  ç¥ç»ç½‘ç»œæ¨¡å—
- **å±‚ç±»å‹**: Linear(å…¨è¿æ¥)ã€Conv2d(å·ç§¯)ã€BatchNorm1d(æ‰¹å½’ä¸€åŒ–)ã€Dropoutã€RNNã€LSTM
- **æ¿€æ´»å‡½æ•°å±‚**: ReLUã€Sigmoidã€Tanhã€Softmax
- **å®¹å™¨**: Sequential é¡ºåºå®¹å™¨
- **æŸå¤±å‡½æ•°**: MSELossã€CrossEntropyLossã€BCEWithLogitsLossã€KLDivergenceLoss
- **ä¼˜åŒ–å™¨**: SGDã€Adamï¼ˆæ”¯æŒåŠ¨é‡å’Œå­¦ä¹ ç‡è°ƒåº¦ï¼‰
- **é«˜çº§æ¨¡å—**: BERTã€GPTã€Transformerã€æ³¨æ„åŠ›æœºåˆ¶ã€çŸ¥è¯†è’¸é¦

### ğŸ’¾ å†…å­˜ç®¡ç†
- **å†…å­˜æ± **: é«˜æ•ˆçš„å†…å­˜åˆ†é…å’Œå›æ”¶
- **å†…å­˜ç›‘æ§**: å®æ—¶è·Ÿè¸ªå†…å­˜ä½¿ç”¨æƒ…å†µ
- **è‡ªåŠ¨æ¸…ç†**: ä¸Šä¸‹æ–‡ç®¡ç†å™¨è‡ªåŠ¨é‡Šæ”¾å†…å­˜
- **æ•°æ®ç±»å‹ç®¡ç†**: æ”¯æŒå¤šç§æ•°æ®ç±»å‹å’Œç±»å‹è½¬æ¢
- **å¼ é‡æ³¨å†Œè¡¨**: è‡ªåŠ¨è·Ÿè¸ªå’Œç®¡ç†å¼ é‡å¯¹è±¡
- **è‡ªåŠ¨å†…å­˜ç®¡ç†**: æ™ºèƒ½å†…å­˜æ¸…ç†å’Œä¼˜åŒ–

### âš¡ GPUåŠ é€Ÿæ”¯æŒ
- **CUDAæ”¯æŒ**: åŸºäºCuPyçš„GPUåŠ é€Ÿ
- **è®¾å¤‡ç®¡ç†**: CPU/GPUä¹‹é—´çš„æ•°æ®ä¼ è¾“
- **å†…å­˜ä¼˜åŒ–**: GPUå†…å­˜æ± ç®¡ç†å’Œé™åˆ¶è®¾ç½®
- **æ··åˆç²¾åº¦**: æ”¯æŒä¸åŒç²¾åº¦çš„æ•°å€¼è®¡ç®—

### ğŸ”§ æ•°æ®å¤„ç†
- **æ•°æ®åŠ è½½**: æ”¯æŒè‡ªå®šä¹‰æ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
- **æ–‡æœ¬å¤„ç†**: å†…ç½®åˆ†è¯å™¨å’Œè¯æ±‡è¡¨ç®¡ç†
- **è¯å‘é‡**: Word2VecåµŒå…¥æ”¯æŒ

## ğŸ“¦ å®‰è£…è¦æ±‚

### åŸºç¡€ä¾èµ–
```bash
pip install numpy
pip install psutil
pip install matplotlib  # å¯é€‰ï¼Œç”¨äºå¯è§†åŒ–
pip install scikit-learn  # å¯é€‰ï¼Œç”¨äºæ•°æ®é¢„å¤„ç†
```

### GPUæ”¯æŒï¼ˆå¯é€‰ï¼‰
```bash
pip install cupy-cuda11x  # æ ¹æ®CUDAç‰ˆæœ¬é€‰æ‹©ï¼Œæ”¯æŒCUDA 11.x
pip install cupy-cuda12x  # æˆ–æ”¯æŒCUDA 12.x
```

### å¼€å‘ä¾èµ–
```bash
pip install jupyter  # ç”¨äºè¿è¡Œç¤ºä¾‹notebook
pip install pytest   # ç”¨äºè¿è¡Œæµ‹è¯•
```

## ğŸ¯ å¿«é€Ÿå¼€å§‹

### åŸºç¡€å¼ é‡æ“ä½œ

```python
from core.tensor import Tensor, tensor, randn, zeros, ones

# åˆ›å»ºå¼ é‡
a = tensor([[1, 2, 3], [4, 5, 6]], requires_grad=True)
b = tensor([[2, 3], [4, 5], [6, 7]], requires_grad=True)

# çŸ©é˜µä¹˜æ³•
c = a @ b
print(c)

# åå‘ä¼ æ’­
loss = c.sum()
loss.backward()
print(a.grad.data)  # æŸ¥çœ‹æ¢¯åº¦
```

### ç¥ç»ç½‘ç»œè®­ç»ƒ

```python
from core.nn.tensor_nn import Sequential, Linear, ReLU, MSELoss, Adam
from core.tensor import randn

# åˆ›å»ºæ¨¡å‹
model = Sequential(
    Linear(10, 20),
    ReLU(),
    Linear(20, 1)
)

# åˆ›å»ºæ•°æ®
X = randn(100, 10)
y = randn(100, 1)

# å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
criterion = MSELoss()
optimizer = Adam(model.parameters(), lr=0.01)

# è®­ç»ƒå¾ªç¯
for epoch in range(100):
    # å‰å‘ä¼ æ’­
    output = model(X)
    loss = criterion(output, y)
    
    # åå‘ä¼ æ’­
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    if epoch % 10 == 0:
        print(f'Epoch {epoch}, Loss: {loss.data:.4f}')
```

### GPUåŠ é€Ÿ

```python
from core.tensor import tensor

# åˆ›å»ºCPUå¼ é‡
cpu_tensor = tensor([[1, 2], [3, 4]], requires_grad=True)

# ç§»åŠ¨åˆ°GPU
gpu_tensor = cpu_tensor.cuda()

# GPUä¸Šè¿›è¡Œè¿ç®—
result = gpu_tensor @ gpu_tensor.T

# ç§»å›CPU
cpu_result = result.cpu()
```

### å†…å­˜ç®¡ç†

```python
from core.utils.memory_utils import memory_context, memory_summary

# ä½¿ç”¨å†…å­˜ä¸Šä¸‹æ–‡ç®¡ç†å™¨
with memory_context(monitor=True, clear_on_exit=True):
    large_tensor = randn(1000, 1000)
    # è‡ªåŠ¨æ¸…ç†å†…å­˜

# æŸ¥çœ‹å†…å­˜ä½¿ç”¨æƒ…å†µ
memory_summary()
```

## ğŸ“š è¯¦ç»†æ–‡æ¡£

### Tensor ç±»

`Tensor` æ˜¯æ¡†æ¶çš„æ ¸å¿ƒæ•°æ®ç»“æ„ï¼Œæ”¯æŒå¤šç»´æ•°ç»„æ“ä½œå’Œè‡ªåŠ¨å¾®åˆ†ã€‚

#### åˆ›å»ºå¼ é‡
```python
from core.tensor import Tensor, tensor, zeros, ones, randn, eye

# ä»æ•°æ®åˆ›å»º
a = tensor([1, 2, 3], requires_grad=True)
b = tensor([[1, 2], [3, 4]], device='cuda')

# ç‰¹æ®Šå¼ é‡
zeros_tensor = zeros(3, 4)
ones_tensor = ones(2, 3)
random_tensor = randn(5, 5)
identity_matrix = eye(4)
```

#### å¼ é‡å±æ€§
```python
print(a.shape)      # å½¢çŠ¶
print(a.ndim)       # ç»´åº¦æ•°
print(a.size)       # å…ƒç´ æ€»æ•°
print(a.dtype)      # æ•°æ®ç±»å‹
print(a.device)     # è®¾å¤‡
```

#### æ•°å­¦è¿ç®—
```python
# åŸºç¡€è¿ç®—
c = a + b           # åŠ æ³•
c = a * b           # ä¹˜æ³•
c = a / b           # é™¤æ³•
c = a ** 2          # å¹‚è¿ç®—

# çŸ©é˜µè¿ç®—
c = a @ b           # çŸ©é˜µä¹˜æ³•
c = a.T             # è½¬ç½®

# èšåˆè¿ç®—
c = a.sum()         # æ±‚å’Œ
c = a.mean(axis=1)  # æŒ‰è½´æ±‚å‡å€¼

# æ¿€æ´»å‡½æ•°
c = a.relu()        # ReLU
c = a.sigmoid()     # Sigmoid
c = a.tanh()        # Tanh
```

#### å½¢çŠ¶æ“ä½œ
```python
# æ”¹å˜å½¢çŠ¶
b = a.reshape(2, 3)

# è½¬ç½®
b = a.transpose()
b = a.T

# ç»´åº¦æ“ä½œ
b = a.sum(axis=0, keepdims=True)
```

### ç¥ç»ç½‘ç»œæ¨¡å—

#### å±‚å®šä¹‰
```python
from core.nn.tensor_nn import Linear, ReLU, Sigmoid, BatchNorm1d, Dropout
from core.nn.modules.conv import Conv2d
from core.nn.modules.rnn import RNN, LSTM

# çº¿æ€§å±‚
linear = Linear(in_features=10, out_features=5)

# æ¿€æ´»å‡½æ•°
relu = ReLU()
sigmoid = Sigmoid()

# æ‰¹å½’ä¸€åŒ–
bn = BatchNorm1d(num_features=5)

# Dropout
dropout = Dropout(p=0.5)

# å·ç§¯å±‚
conv = Conv2d(in_channels=3, out_channels=64, kernel_size=3)

# å¾ªç¯ç¥ç»ç½‘ç»œ
rnn = RNN(input_size=128, hidden_size=256, num_layers=2)
lstm = LSTM(input_size=128, hidden_size=256, num_layers=2)
```

#### æ¨¡å‹æ„å»º
```python
from core.nn.tensor_nn import Sequential

model = Sequential(
    Linear(784, 128),
    ReLU(),
    BatchNorm1d(128),
    Dropout(0.2),
    Linear(128, 64),
    ReLU(),
    Linear(64, 10)
)
```

#### æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
```python
from core.nn.tensor_nn import MSELoss, CrossEntropyLoss, SGD, Adam
from core.nn.loss import BCEWithLogitsLoss

# æŸå¤±å‡½æ•°
mse_loss = MSELoss()
ce_loss = CrossEntropyLoss()
bce_loss = BCEWithLogitsLoss()

# ä¼˜åŒ–å™¨
sgd = SGD(model.parameters(), lr=0.01, momentum=0.9)
adam = Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999))
```

### å†…å­˜ç®¡ç†

#### å†…å­˜ç›‘æ§
```python
from core.utils.memory_utils import get_memory_monitor, memory_summary

monitor = get_memory_monitor()
usage = monitor.get_memory_usage()
memory_summary()
```

#### å†…å­˜æ± 
```python
from core.utils.memory_utils import get_memory_pool

pool = get_memory_pool('cpu')
allocated_memory = pool.allocate(1000000, np.float32)
```

#### ä¸Šä¸‹æ–‡ç®¡ç†å™¨
```python
from core.utils.memory_utils import memory_context

with memory_context(monitor=True, clear_on_exit=True):
    # åœ¨æ­¤ä¸Šä¸‹æ–‡ä¸­çš„å†…å­˜ä½¿ç”¨ä¼šè¢«ç›‘æ§
    # é€€å‡ºæ—¶è‡ªåŠ¨æ¸…ç†
    pass
```

## ğŸ¨ å®Œæ•´ç¤ºä¾‹

æŸ¥çœ‹ `test/unit/demo_basic_tensor_operations.py` æ–‡ä»¶è·å–å®Œæ•´çš„ä½¿ç”¨ç¤ºä¾‹ï¼ŒåŒ…æ‹¬ï¼š

1. åŸºç¡€å¼ é‡æ“ä½œæ¼”ç¤º
2. æ¿€æ´»å‡½æ•°å¯è§†åŒ–
3. ç¥ç»ç½‘ç»œè®­ç»ƒï¼ˆXORé—®é¢˜ï¼‰
4. æ‰¹é‡æ“ä½œ
5. GPUæ“ä½œ
6. å†…å­˜ç®¡ç†
7. æ€§èƒ½åŸºå‡†æµ‹è¯•

è¿è¡Œç¤ºä¾‹ï¼š
```bash
python test/unit/demo_basic_tensor_operations.py
```

## ğŸ—ï¸ æ¶æ„è®¾è®¡

### æ ¸å¿ƒç»„ä»¶

```
EasyTensor/
â”œâ”€â”€ core/                    # æ ¸å¿ƒæ¨¡å—
â”‚   â”œâ”€â”€ tensor.py           # æ ¸å¿ƒTensorç±»
â”‚   â”œâ”€â”€ device.py           # è®¾å¤‡ç®¡ç†
â”‚   â”œâ”€â”€ model_io.py         # æ¨¡å‹ä¿å­˜å’ŒåŠ è½½
â”‚   â”œâ”€â”€ nn/                 # ç¥ç»ç½‘ç»œæ¨¡å—
â”‚   â”‚   â”œâ”€â”€ tensor_nn.py    # åŸºç¡€ç¥ç»ç½‘ç»œå±‚
â”‚   â”‚   â”œâ”€â”€ modules/        # å…·ä½“æ¨¡å—å®ç°
â”‚   â”‚   â”‚   â”œâ”€â”€ conv.py     # å·ç§¯å±‚
â”‚   â”‚   â”‚   â”œâ”€â”€ embedding.py # åµŒå…¥å±‚
â”‚   â”‚   â”‚   â”œâ”€â”€ pooling.py  # æ± åŒ–å±‚
â”‚   â”‚   â”‚   â””â”€â”€ rnn.py      # å¾ªç¯ç¥ç»ç½‘ç»œ
â”‚   â”‚   â”œâ”€â”€ attention.py    # æ³¨æ„åŠ›æœºåˆ¶
â”‚   â”‚   â”œâ”€â”€ bert_gpt.py     # BERT/GPTæ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ transform.py    # Transformeræ¨¡å‹
â”‚   â”‚   â””â”€â”€ distill.py      # çŸ¥è¯†è’¸é¦
â”‚   â”œâ”€â”€ data/               # æ•°æ®å¤„ç†
â”‚   â”‚   â”œâ”€â”€ dataloader.py   # æ•°æ®åŠ è½½å™¨
â”‚   â”‚   â””â”€â”€ word2vec.py     # è¯å‘é‡
â”‚   â”œâ”€â”€ optim/              # ä¼˜åŒ–å™¨
â”‚   â”‚   â””â”€â”€ lr_scheduler.py # å­¦ä¹ ç‡è°ƒåº¦å™¨
â”‚   â”œâ”€â”€ utils/              # å·¥å…·æ¨¡å—
â”‚   â”‚   â”œâ”€â”€ memory_utils.py # å†…å­˜ç®¡ç†
â”‚   â”‚   â”œâ”€â”€ tokenizer.py    # åˆ†è¯å™¨
â”‚   â”‚   â””â”€â”€ serialization.py # åºåˆ—åŒ–å·¥å…·
â”‚   â””â”€â”€ v1/                 # å…¼å®¹å±‚
â”‚       â”œâ”€â”€ engine.py       # åŸå§‹Nodeç±»
â”‚       â”œâ”€â”€ nn.py           # åŸå§‹ç¥ç»ç½‘ç»œæ¨¡å—
â”‚       â””â”€â”€ optim/          # åŸå§‹ä¼˜åŒ–å™¨
â”œâ”€â”€ test/                   # æµ‹è¯•å’Œç¤ºä¾‹
â”‚   â”œâ”€â”€ unit/               # å•å…ƒæµ‹è¯•
â”‚   â”œâ”€â”€ forward/            # å‰å‘ä¼ æ’­æµ‹è¯•
â”‚   â””â”€â”€ network/            # ç½‘ç»œæµ‹è¯•
â””â”€â”€ biz/                    # ä¸šåŠ¡ç¤ºä¾‹
    â””â”€â”€ cnn.py              # CNNç¤ºä¾‹
```

### è®¾è®¡åŸåˆ™

1. **å‘åå…¼å®¹**: ä¿ç•™åŸæœ‰çš„Nodeå’Œv1æ¨¡å—æ¥å£ï¼Œç¡®ä¿å¹³æ»‘å‡çº§
2. **æ¨¡å—åŒ–**: æ¯ä¸ªåŠŸèƒ½æ¨¡å—ç‹¬ç«‹ï¼Œä¾¿äºç»´æŠ¤å’Œæ‰©å±•
3. **æ€§èƒ½ä¼˜åŒ–**: æ”¯æŒGPUåŠ é€Ÿå’Œæ™ºèƒ½å†…å­˜ç®¡ç†
4. **æ˜“ç”¨æ€§**: ç±»ä¼¼PyTorchçš„APIè®¾è®¡ï¼Œé™ä½å­¦ä¹ æˆæœ¬
5. **å¯æ‰©å±•æ€§**: æ”¯æŒè‡ªå®šä¹‰å±‚ã€ä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°
6. **å†…å­˜æ•ˆç‡**: æ™ºèƒ½å†…å­˜æ± å’Œè‡ªåŠ¨åƒåœ¾å›æ”¶

## ğŸ”¬ æ€§èƒ½å¯¹æ¯”

| æ“ä½œç±»å‹ | çŸ©é˜µå¤§å° | NumPyæ—¶é—´ | EasyTensoræ—¶é—´ | PyTorchæ—¶é—´ | ç›¸å¯¹æ€§èƒ½ |
|----------|----------|-----------|----------------|-------------|----------|
| çŸ©é˜µä¹˜æ³• | 100Ã—100  | 0.001s    | 0.002s         | 0.001s      | 2.0x     |
| çŸ©é˜µä¹˜æ³• | 500Ã—500  | 0.050s    | 0.065s         | 0.045s      | 1.3x     |
| çŸ©é˜µä¹˜æ³• | 1000Ã—1000| 0.200s    | 0.280s         | 0.180s      | 1.4x     |
| GPUçŸ©é˜µä¹˜æ³• | 1000Ã—1000| N/A | 0.120s | 0.080s | 1.5x |

*æ³¨ï¼šæ€§èƒ½ä¼šå› ç¡¬ä»¶å’Œå…·ä½“å®ç°è€Œå¼‚ã€‚è¿è¡Œ `test/æ¨ªå‘å¯¹æ¯”æµ‹è¯•.py` è·å–è¯¦ç»†æ€§èƒ½å¯¹æ¯”*

## ğŸ› å·²çŸ¥é™åˆ¶

1. **éƒ¨åˆ†å®ç°**: æŸäº›é«˜çº§åŠŸèƒ½ä»åœ¨å¼€å‘ä¸­
2. **æ€§èƒ½**: ç›¸æ¯”ä¸“ä¸šæ¡†æ¶ï¼ˆPyTorch/TensorFlowï¼‰æ€§èƒ½ä»æœ‰å·®è·
3. **åˆ†å¸ƒå¼**: æš‚ä¸æ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒ
4. **ç”Ÿæ€**: ç¼ºå°‘ä¸°å¯Œçš„é¢„è®­ç»ƒæ¨¡å‹å’Œå·¥å…·é“¾

## ğŸ§ª æµ‹è¯•å’Œç¤ºä¾‹

### è¿è¡Œæµ‹è¯•
```bash
# åŸºç¡€åŠŸèƒ½æµ‹è¯•
python test/unit/demo_basic_tensor_operations.py

# æ€§èƒ½å¯¹æ¯”æµ‹è¯•
python test/æ¨ªå‘å¯¹æ¯”æµ‹è¯•.py

# å¼•æ“æµ‹è¯•
python test/å¼•æ“æµ‹è¯•.py

# ç½‘ç»œæµ‹è¯•
python test/unit/deep_network_test.py
```

### é«˜çº§ç¤ºä¾‹
```bash
# BERT/GPTç¤ºä¾‹
python core/nn/bert_gpt_example.py

# çŸ¥è¯†è’¸é¦ç¤ºä¾‹
python core/nn/distill.py

# CNNç¤ºä¾‹
python biz/cnn.py
```

## ğŸ¤ è´¡çŒ®æŒ‡å—

æ¬¢è¿æäº¤Pull Requestæ¥æ”¹è¿›è¿™ä¸ªæ¡†æ¶ï¼

1. Fork è¿™ä¸ªé¡¹ç›®
2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯ (`git checkout -b feature/AmazingFeature`)
3. æäº¤æ›´æ”¹ (`git commit -m 'Add some AmazingFeature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/AmazingFeature`)
5. å¼€å¯ Pull Request

## ğŸ“„ è®¸å¯è¯

è¿™ä¸ªé¡¹ç›®ä½¿ç”¨ MIT è®¸å¯è¯ - æŸ¥çœ‹ [LICENSE](LICENSE) æ–‡ä»¶äº†è§£è¯¦æƒ…ã€‚

## ğŸ™ è‡´è°¢

- **NumPyå›¢é˜Ÿ** - æä¾›äº†å¼ºå¤§çš„æ•°å€¼è®¡ç®—åŸºç¡€
- **CuPyå›¢é˜Ÿ** - æä¾›äº†GPUåŠ é€Ÿæ”¯æŒ
- **PyTorchå›¢é˜Ÿ** - ä¸ºAPIè®¾è®¡æä¾›äº†çµæ„Ÿ
- **TensorFlowå›¢é˜Ÿ** - ä¸ºæ¶æ„è®¾è®¡æä¾›äº†å‚è€ƒ
- **å¼€æºç¤¾åŒº** - æ„Ÿè°¢æ‰€æœ‰è´¡çŒ®è€…å’Œç”¨æˆ·çš„æ”¯æŒ

## ğŸ“ è”ç³»æ–¹å¼

å¦‚æœä½ æœ‰ä»»ä½•é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·é€šè¿‡ä»¥ä¸‹æ–¹å¼è”ç³»ï¼š

- æäº¤ Issue
- å‘é€é‚®ä»¶
- å‚ä¸è®¨è®º

---

**Happy Deep Learning with EasyTensor! ğŸ§ âœ¨**

*è®©æ·±åº¦å­¦ä¹ å˜å¾—æ›´ç®€å•ã€æ›´é«˜æ•ˆã€æ›´æœ‰è¶£ï¼*