import numpy as np
import matplotlib.pyplot as plt
import time
import random
import torch
import torch.nn as nn
import torch.optim as optim
from core.v1.nn import MLP
from core.v1.engine import Node

# è®¾ç½®ä¸­æ–‡å­—ä½“æ˜¾ç¤º
plt.rcParams['font.sans-serif'] = ['Hiragino Sans GB', 'SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False


class SimpleEngineComparator:
    """ç®€åŒ–ç‰ˆå¼•æ“å¯¹æ¯”å™¨ - é¿å…å¤æ‚çš„æƒé‡åŒæ­¥"""

    def __init__(self, X_train, y_train, X_test, y_test):
        self.X_train = X_train
        self.y_train = y_train
        self.X_test = X_test
        self.y_test = y_test

    def set_seeds(self, seed):
        """è®¾ç½®éšæœºç§å­"""
        random.seed(seed)
        np.random.seed(seed)
        torch.manual_seed(seed)

    def create_torch_network(self, architecture):
        """åˆ›å»ºPyTorchç½‘ç»œ"""
        nin, hidden_layers, nout = architecture
        layers = []

        # æ„å»ºå±‚
        prev_size = nin
        for hidden_size in hidden_layers:
            layers.append(nn.Linear(prev_size, hidden_size))
            layers.append(nn.ReLU())
            prev_size = hidden_size

        # è¾“å‡ºå±‚
        layers.append(nn.Linear(prev_size, nout))

        return nn.Sequential(*layers)

    def train_custom_engine(self, architecture, learning_rate=0.01, epochs=100, seed=42):
        """è®­ç»ƒè‡ªåˆ¶å¼•æ“"""
        self.set_seeds(seed)

        nin, hidden_layers, nout = architecture
        net = MLP(nin, hidden_layers + [nout])

        losses = []
        start_time = time.time()

        for epoch in range(epochs):
            total_loss = Node(0.0)

            for i in range(len(self.X_train)):
                x = [Node(float(self.X_train[i, 0])), Node(float(self.X_train[i, 1]))]
                y_true = Node(float(self.y_train[i]))

                y_pred = net(x)
                diff = y_pred - y_true
                loss = diff * diff
                total_loss = total_loss + loss

            avg_loss = total_loss * Node(1.0 / len(self.X_train))
            losses.append(avg_loss.data)

            # åå‘ä¼ æ’­å’Œå‚æ•°æ›´æ–°
            net.zero_grad()
            avg_loss.backward()

            for param in net.parameters():
                param.data -= learning_rate * param.grad

        training_time = time.time() - start_time

        # æµ‹è¯•é›†é¢„æµ‹
        test_predictions = []
        for i in range(len(self.X_test)):
            x = [Node(float(self.X_test[i, 0])), Node(float(self.X_test[i, 1]))]
            pred = net(x)
            test_predictions.append(pred.data)

        test_mse = np.mean((np.array(test_predictions) - self.y_test) ** 2)

        return {
            'network': net,
            'losses': losses,
            'training_time': training_time,
            'test_mse': test_mse,
            'test_predictions': np.array(test_predictions)
        }

    def train_torch_engine(self, architecture, learning_rate=0.01, epochs=100, seed=42):
        """è®­ç»ƒPyTorchå¼•æ“"""
        self.set_seeds(seed)

        net = self.create_torch_network(architecture)

        X_torch = torch.FloatTensor(self.X_train)
        y_torch = torch.FloatTensor(self.y_train).reshape(-1, 1)
        X_test_torch = torch.FloatTensor(self.X_test)

        optimizer = optim.SGD(net.parameters(), lr=learning_rate)
        criterion = nn.MSELoss()

        losses = []
        start_time = time.time()

        for epoch in range(epochs):
            y_pred = net(X_torch)
            loss = criterion(y_pred, y_torch)
            losses.append(loss.item())

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

        training_time = time.time() - start_time

        # æµ‹è¯•é›†é¢„æµ‹
        with torch.no_grad():
            test_predictions = net(X_test_torch).numpy().flatten()

        test_mse = np.mean((test_predictions - self.y_test) ** 2)

        return {
            'network': net,
            'losses': losses,
            'training_time': training_time,
            'test_mse': test_mse,
            'test_predictions': test_predictions
        }

    def gradient_consistency_test(self, custom_net, torch_net, test_point=[1.0, -0.5]):
        """æ¢¯åº¦ä¸€è‡´æ€§æµ‹è¯•"""
        # è‡ªåˆ¶å¼•æ“æ¢¯åº¦
        x_custom = [Node(test_point[0]), Node(test_point[1])]
        for param in custom_net.parameters():
            param.grad = 0

        y_custom = custom_net(x_custom)
        y_custom.backward()

        custom_grads = [x_custom[0].grad, x_custom[1].grad]

        # PyTorchæ¢¯åº¦
        x_torch = torch.tensor([test_point], requires_grad=True, dtype=torch.float32)
        y_torch = torch_net(x_torch)
        y_torch.sum().backward()  # ä½¿ç”¨sum()ç¡®ä¿æ ‡é‡

        torch_grads = x_torch.grad[0].tolist()

        grad_diff = [abs(custom_grads[i] - torch_grads[i]) for i in range(2)]

        return {
            'custom_grads': custom_grads,
            'torch_grads': torch_grads,
            'differences': grad_diff,
            'max_diff': max(grad_diff)
        }

    def comprehensive_comparison(self, architecture=[2, [8, 8], 1],
                                 learning_rate=0.01, epochs=50, n_trials=3):
        """ç»¼åˆå¯¹æ¯”åˆ†æ"""

        print("=" * 60)
        print("è‡ªåˆ¶å¾®åˆ†å¼•æ“ vs PyTorch ç»¼åˆå¯¹æ¯”åˆ†æ")
        print("=" * 60)
        print(f"ç½‘ç»œæ¶æ„: {architecture[0]}è¾“å…¥ -> {architecture[1]} -> {architecture[2]}è¾“å‡º")
        print(f"è®­ç»ƒå‚æ•°: å­¦ä¹ ç‡={learning_rate}, è½®æ•°={epochs}, è¯•éªŒæ¬¡æ•°={n_trials}")
        print()

        all_results = {
            'custom': {'losses': [], 'times': [], 'mse': [], 'final_losses': []},
            'torch': {'losses': [], 'times': [], 'mse': [], 'final_losses': []},
            'gradient_tests': []
        }

        for trial in range(n_trials):
            print(f">>> è¯•éªŒ {trial + 1}/{n_trials}")

            # ä½¿ç”¨ä¸åŒç§å­è¿›è¡Œè®­ç»ƒ
            seed = 42 + trial

            # è®­ç»ƒè‡ªåˆ¶å¼•æ“
            print("  è®­ç»ƒè‡ªåˆ¶å¼•æ“...")
            custom_result = self.train_custom_engine(architecture, learning_rate, epochs, seed)

            # è®­ç»ƒPyTorchå¼•æ“
            print("  è®­ç»ƒPyTorchå¼•æ“...")
            torch_result = self.train_torch_engine(architecture, learning_rate, epochs, seed)

            # æ¢¯åº¦ä¸€è‡´æ€§æµ‹è¯•
            print("  æ¢¯åº¦ä¸€è‡´æ€§æµ‹è¯•...")
            grad_result = self.gradient_consistency_test(
                custom_result['network'], torch_result['network']
            )

            # æ”¶é›†ç»“æœ
            all_results['custom']['losses'].append(custom_result['losses'])
            all_results['custom']['times'].append(custom_result['training_time'])
            all_results['custom']['mse'].append(custom_result['test_mse'])
            all_results['custom']['final_losses'].append(custom_result['losses'][-1])

            all_results['torch']['losses'].append(torch_result['losses'])
            all_results['torch']['times'].append(torch_result['training_time'])
            all_results['torch']['mse'].append(torch_result['test_mse'])
            all_results['torch']['final_losses'].append(torch_result['losses'][-1])

            all_results['gradient_tests'].append(grad_result)

            # æ‰“å°å•æ¬¡ç»“æœ
            print(f"    æœ€ç»ˆæŸå¤±: è‡ªåˆ¶={custom_result['losses'][-1]:.6f}, "
                  f"PyTorch={torch_result['losses'][-1]:.6f}")
            print(f"    æµ‹è¯•MSE:  è‡ªåˆ¶={custom_result['test_mse']:.6f}, "
                  f"PyTorch={torch_result['test_mse']:.6f}")
            print(f"    è®­ç»ƒæ—¶é—´: è‡ªåˆ¶={custom_result['training_time']:.3f}s, "
                  f"PyTorch={torch_result['training_time']:.3f}s")
            print(f"    æ¢¯åº¦å·®å¼‚: {grad_result['max_diff']:.8f}")
            print()

        # ç»Ÿè®¡åˆ†æ
        self.statistical_analysis(all_results)

        # å¯è§†åŒ–ç»“æœ
        self.visualize_results(all_results, epochs)

        return all_results

    def statistical_analysis(self, results):
        """ç»Ÿè®¡åˆ†æç»“æœ"""
        print("=" * 40 + " ç»Ÿè®¡åˆ†æ " + "=" * 40)

        # 1. æ”¶æ•›æ€§åˆ†æ
        print("\n1. æ”¶æ•›æ€§åˆ†æ:")
        custom_final_losses = results['custom']['final_losses']
        torch_final_losses = results['torch']['final_losses']

        custom_mean = np.mean(custom_final_losses)
        custom_std = np.std(custom_final_losses)
        torch_mean = np.mean(torch_final_losses)
        torch_std = np.std(torch_final_losses)

        print(f"   æœ€ç»ˆæŸå¤±ç»Ÿè®¡:")
        print(f"     è‡ªåˆ¶å¼•æ“: {custom_mean:.6f} Â± {custom_std:.6f}")
        print(f"     PyTorch:  {torch_mean:.6f} Â± {torch_std:.6f}")
        print(f"     å¹³å‡å·®å¼‚: {abs(custom_mean - torch_mean):.6f}")

        # 2. é¢„æµ‹æ€§èƒ½åˆ†æ
        print("\n2. é¢„æµ‹æ€§èƒ½åˆ†æ:")
        custom_mse = results['custom']['mse']
        torch_mse = results['torch']['mse']

        custom_mse_mean = np.mean(custom_mse)
        custom_mse_std = np.std(custom_mse)
        torch_mse_mean = np.mean(torch_mse)
        torch_mse_std = np.std(torch_mse)

        print(f"   æµ‹è¯•MSEç»Ÿè®¡:")
        print(f"     è‡ªåˆ¶å¼•æ“: {custom_mse_mean:.6f} Â± {custom_mse_std:.6f}")
        print(f"     PyTorch:  {torch_mse_mean:.6f} Â± {torch_mse_std:.6f}")
        print(f"     ç›¸å¯¹å·®å¼‚: {abs(custom_mse_mean - torch_mse_mean) / torch_mse_mean * 100:.2f}%")

        # 3. è®¡ç®—æ•ˆç‡åˆ†æ
        print("\n3. è®¡ç®—æ•ˆç‡åˆ†æ:")
        custom_times = results['custom']['times']
        torch_times = results['torch']['times']

        custom_time_mean = np.mean(custom_times)
        torch_time_mean = np.mean(torch_times)
        speed_ratio = custom_time_mean / torch_time_mean

        print(f"   å¹³å‡è®­ç»ƒæ—¶é—´:")
        print(f"     è‡ªåˆ¶å¼•æ“: {custom_time_mean:.3f}s")
        print(f"     PyTorch:  {torch_time_mean:.3f}s")
        print(f"     é€Ÿåº¦æ¯”ç‡: {speed_ratio:.2f}x (è‡ªåˆ¶/PyTorch)")

        # 4. æ¢¯åº¦ä¸€è‡´æ€§åˆ†æ
        print("\n4. æ¢¯åº¦ä¸€è‡´æ€§åˆ†æ:")
        grad_diffs = [g['max_diff'] for g in results['gradient_tests']]
        grad_mean = np.mean(grad_diffs)
        grad_std = np.std(grad_diffs)

        print(f"   æœ€å¤§æ¢¯åº¦å·®å¼‚: {grad_mean:.8f} Â± {grad_std:.8f}")

        consistency_rate = sum(1 for d in grad_diffs if d < 1e-6) / len(grad_diffs)
        print(f"   é«˜ä¸€è‡´æ€§æ¯”ç‡: {consistency_rate * 100:.1f}% (å·®å¼‚ < 1e-6)")

        # 5. ç»¼åˆè¯„ä¼°
        print("\n" + "=" * 20 + " ç»¼åˆè¯„ä¼° " + "=" * 20)

        # è¯„ä¼°æ ‡å‡†
        loss_consistent = abs(custom_mean - torch_mean) < 1e-3
        gradient_consistent = grad_mean < 1e-6
        performance_reasonable = abs(custom_mse_mean - torch_mse_mean) / torch_mse_mean < 0.1

        print(f"âœ“ æŸå¤±ä¸€è‡´æ€§: {'é€šè¿‡' if loss_consistent else 'æœªé€šè¿‡'}")
        print(f"âœ“ æ¢¯åº¦ä¸€è‡´æ€§: {'é€šè¿‡' if gradient_consistent else 'æœªé€šè¿‡'}")
        print(f"âœ“ æ€§èƒ½åˆç†æ€§: {'é€šè¿‡' if performance_reasonable else 'æœªé€šè¿‡'}")

        if loss_consistent and gradient_consistent and performance_reasonable:
            print("\nğŸ‰ ç»“è®º: è‡ªåˆ¶å¼•æ“å®ç°æ­£ç¡®ï¼Œä¸PyTorchè¡¨ç°é«˜åº¦ä¸€è‡´ï¼")
            print("æ¨èä½¿ç”¨åœºæ™¯:")
            print("  - å­¦ä¹ æ·±åº¦å­¦ä¹ åŸç†å’Œè‡ªåŠ¨å¾®åˆ†æœºåˆ¶")
            print("  - ç†è§£ç¥ç»ç½‘ç»œçš„åº•å±‚å®ç°ç»†èŠ‚")
            print("  - åŸå‹éªŒè¯å’Œç®—æ³•ç ”ç©¶")
        else:
            print("\nâš ï¸  ç»“è®º: å­˜åœ¨ä¸€å®šå·®å¼‚ï¼Œå»ºè®®è¿›ä¸€æ­¥è°ƒè¯•")
            if not loss_consistent:
                print("  - æ£€æŸ¥æŸå¤±å‡½æ•°è®¡ç®—")
            if not gradient_consistent:
                print("  - æ£€æŸ¥åå‘ä¼ æ’­å®ç°")
            if not performance_reasonable:
                print("  - æ£€æŸ¥ç½‘ç»œç»“æ„å¯¹åº”å…³ç³»")

    def visualize_results(self, results, epochs):
        """å¯è§†åŒ–ç»“æœ"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))

        # 1. è®­ç»ƒæŸå¤±æ›²çº¿å¯¹æ¯”
        ax1 = axes[0, 0]
        for i, (custom_losses, torch_losses) in enumerate(zip(
                results['custom']['losses'], results['torch']['losses']
        )):
            alpha = 0.7 if len(results['custom']['losses']) > 1 else 1.0
            ax1.plot(custom_losses, label=f'è‡ªåˆ¶å¼•æ“ è¯•éªŒ{i + 1}' if i == 0 else '',
                     color='blue', alpha=alpha, linewidth=1)
            ax1.plot(torch_losses, label=f'PyTorch è¯•éªŒ{i + 1}' if i == 0 else '',
                     color='red', alpha=alpha, linewidth=1)

        ax1.set_xlabel('è®­ç»ƒè½®æ•°')
        ax1.set_ylabel('æŸå¤±å€¼')
        ax1.set_title('è®­ç»ƒæŸå¤±æ›²çº¿å¯¹æ¯”')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        ax1.set_yscale('log')

        # 2. æœ€ç»ˆæŸå¤±å¯¹æ¯”
        ax2 = axes[0, 1]
        custom_final = results['custom']['final_losses']
        torch_final = results['torch']['final_losses']

        x = np.arange(len(custom_final))
        width = 0.35

        ax2.bar(x - width / 2, custom_final, width, label='è‡ªåˆ¶å¼•æ“', alpha=0.7)
        ax2.bar(x + width / 2, torch_final, width, label='PyTorch', alpha=0.7)

        ax2.set_xlabel('è¯•éªŒç¼–å·')
        ax2.set_ylabel('æœ€ç»ˆæŸå¤±å€¼')
        ax2.set_title('æœ€ç»ˆæŸå¤±å¯¹æ¯”')
        ax2.legend()
        ax2.grid(True, alpha=0.3)

        # 3. æµ‹è¯•MSEå¯¹æ¯”
        ax3 = axes[1, 0]
        custom_mse = results['custom']['mse']
        torch_mse = results['torch']['mse']

        ax3.bar(x - width / 2, custom_mse, width, label='è‡ªåˆ¶å¼•æ“', alpha=0.7)
        ax3.bar(x + width / 2, torch_mse, width, label='PyTorch', alpha=0.7)

        ax3.set_xlabel('è¯•éªŒç¼–å·')
        ax3.set_ylabel('æµ‹è¯•MSE')
        ax3.set_title('æµ‹è¯•æ€§èƒ½å¯¹æ¯”')
        ax3.legend()
        ax3.grid(True, alpha=0.3)

        # 4. æ¢¯åº¦å·®å¼‚åˆ†æ
        ax4 = axes[1, 1]
        grad_diffs = [g['max_diff'] for g in results['gradient_tests']]

        ax4.bar(x, grad_diffs, alpha=0.7, color='green')
        ax4.axhline(y=1e-6, color='red', linestyle='--', label='ä¸€è‡´æ€§é˜ˆå€¼ (1e-6)')

        ax4.set_xlabel('è¯•éªŒç¼–å·')
        ax4.set_ylabel('æœ€å¤§æ¢¯åº¦å·®å¼‚')
        ax4.set_title('æ¢¯åº¦ä¸€è‡´æ€§åˆ†æ')
        ax4.legend()
        ax4.grid(True, alpha=0.3)
        ax4.set_yscale('log')

        plt.tight_layout()
        plt.show()


# ä½¿ç”¨ç¤ºä¾‹
def run_comparison_example():
    """è¿è¡Œå¯¹æ¯”ç¤ºä¾‹"""

    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    def target_function(x1, x2):
        return np.sin(x1) * np.cos(x2) + 0.1 * x1 * x2

    # è®­ç»ƒæ•°æ®
    np.random.seed(123)  # å›ºå®šæ•°æ®ç”Ÿæˆç§å­
    X_train = np.random.uniform(-2, 2, (100, 2))
    y_train = target_function(X_train[:, 0], X_train[:, 1])

    # æµ‹è¯•æ•°æ®
    X_test = np.random.uniform(-2, 2, (50, 2))
    y_test = target_function(X_test[:, 0], X_test[:, 1])

    print("æ•°æ®å‡†å¤‡å®Œæˆ:")
    print(f"  è®­ç»ƒé›†å¤§å°: {X_train.shape}")
    print(f"  æµ‹è¯•é›†å¤§å°: {X_test.shape}")
    print(f"  ç›®æ ‡å‡½æ•°: sin(x1) * cos(x2) + 0.1 * x1 * x2")
    print()

    # åˆ›å»ºå¯¹æ¯”å™¨
    comparator = SimpleEngineComparator(X_train, y_train, X_test, y_test)

    # è¿è¡Œå¯¹æ¯”åˆ†æ
    results = comparator.comprehensive_comparison(
        architecture=[2, [8, 8], 1],  # 2è¾“å…¥ -> 8éšè— -> 8éšè— -> 1è¾“å‡º
        learning_rate=0.01,
        epochs=100,
        n_trials=3
    )

    return results


if __name__ == "__main__":
    results = run_comparison_example()
